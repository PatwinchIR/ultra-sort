#include "avx512/utils.h"
#include "common.h"

#ifdef AVX512

/**
 * Load and Store Instructions
 */

template <typename InType, typename RegType>
void AVX512Util::LoadReg(RegType &r, InType *arr) {
  r = *((RegType*)arr);
}

template void AVX512Util::LoadReg<int, __m512i>( __m512i &r, int *arr);
template void AVX512Util::LoadReg<int64_t, __m512i>(__m512i &r, int64_t *arr);
template void AVX512Util::LoadReg<float, __m512>(__m512 &r, float *arr);
template void AVX512Util::LoadReg<double, __m512d>(__m512d &r, double *arr);

template <typename InType, typename RegType>
void AVX512Util::StoreReg(const RegType &r, InType *arr) {
  *((RegType*)arr) = r;
}

template void AVX512Util::StoreReg<int, __m512i>(const __m512i &r, int *arr);
template void AVX512Util::StoreReg<int64_t, __m512i>(const __m512i &r, int64_t *arr);
template void AVX512Util::StoreReg<float, __m512>(const __m512 &r, float *arr);
template void AVX512Util::StoreReg<double, __m512d>(const __m512d &r, double *arr);

// Weird Converter


/**
 * MinMax functions
 */

void AVX512Util::MinMax16(__m512i &a, __m512i &b) {
  __m512i c = a;
  a = _mm512_min_epi32(a, b);
  b = _mm512_max_epi32(c, b);
}

void AVX512Util::MinMax16(const __m512i& a, const __m512i& b,
                          __m512i& minab, __m512i& maxab) {
  minab = _mm512_min_epi32(a, b);
  maxab = _mm512_max_epi32(a, b);
}

void AVX512Util::MinMax16(__m512 &a, __m512 &b) {
  __m512 c = a;
  a = _mm512_min_ps(a, b);
  b = _mm512_max_ps(c, b);
}

void AVX512Util::MinMax16(const __m512& a, const __m512& b,
                          __m512& minab, __m512& maxab) {
  minab = _mm512_min_ps(a, b);
  maxab = _mm512_max_ps(a, b);
}

void AVX512Util::MinMax8(__m512i &a, __m512i &b) {
  __m512i c = a;
  a = _mm512_min_epi64(a, b);
  b = _mm512_max_epi64(c, b);
}

void AVX512Util::MinMax8(const __m512i &a, const __m512i &b,
                         __m512i& minab, __m512i& maxab) {
  minab = _mm512_min_epi64(a, b);
  maxab = _mm512_max_epi64(a, b);
}

void AVX512Util::MinMax8(__m512d &a, __m512d &b) {
  __m512d c = a;
  a = _mm512_min_pd(a, b);
  b = _mm512_max_pd(c, b);
}

void AVX512Util::MinMax8(const __m512d &a, const __m512d &b,
                         __m512d& minab, __m512d& maxab) {
  minab = _mm512_min_pd(a, b);
  maxab = _mm512_max_pd(a, b);
}

/**
 * Bitonic Transpose:
 * 16x16 networks: int32, float32
 * 8x8 networks: int64
 */
void AVX512Util::Transpose8x8(__m512i &row0,
                              __m512i &row1,
                              __m512i &row2,
                              __m512i &row3,
                              __m512i &row4,
                              __m512i &row5,
                              __m512i &row6,
                              __m512i &row7) {
  // TODO: Convert
//  __m512i __t0, __t1, __t2, __t3, __t4, __t5, __t6, __t7;
//  __m512i __tt0, __tt1, __tt2, __tt3, __tt4, __tt5, __tt6, __tt7;
//  __t0 = _mm512_unpacklo_epi64(row0, row1);
//  __t1 = _mm512_unpackhi_epi64(row0, row1);
//  __t2 = _mm512_unpacklo_epi64(row2, row3);
//  __t3 = _mm512_unpackhi_epi64(row2, row3);
//  __t4 = _mm512_unpacklo_epi64(row4, row5);
//  __t5 = _mm512_unpackhi_epi64(row4, row5);
//  __t6 = _mm512_unpacklo_epi64(row6, row7);
//  __t7 = _mm512_unpackhi_epi64(row6, row7);
//  // Note: https://stackoverflow.com/questions/26983569/implications-of-using-mm-shuffle-ps-on-integer-vector
//  // As provided in above link, it is relatively safe/free to run float shuffles on integers(not other way around)
//  __tt0 = _mm512_shuffle_pd(__t0,__t2,_MM_SHUFFLE(1,0,1,0));
//  __tt1 = _mm512_shuffle_pd(__t0,__t2,_MM_SHUFFLE(3,2,3,2));
//  __tt2 = _mm512_shuffle_pd(__t1,__t3,_MM_SHUFFLE(1,0,1,0));
//  __tt3 = _mm512_shuffle_pd(__t1,__t3,_MM_SHUFFLE(3,2,3,2));
//  __tt4 = _mm512_shuffle_pd(__t4,__t6,_MM_SHUFFLE(1,0,1,0));
//  __tt5 = _mm512_shuffle_pd(__t4,__t6,_MM_SHUFFLE(3,2,3,2));
//  __tt6 = _mm512_shuffle_pd(__t5,__t7,_MM_SHUFFLE(1,0,1,0));
//  __tt7 = _mm512_shuffle_pd(__t5,__t7,_MM_SHUFFLE(3,2,3,2));
//  row0 = (__m512i)_mm512_permute2f128_ps(__tt0, __tt4, 0x20);
//  row1 = (__m512i)_mm512_permute2f128_ps(__tt1, __tt5, 0x20);
//  row2 = (__m512i)_mm512_permute2f128_ps(__tt2, __tt6, 0x20);
//  row3 = (__m512i)_mm512_permute2f128_ps(__tt3, __tt7, 0x20);
//  row4 = (__m512i)_mm512_permute2f128_ps(__tt0, __tt4, 0x31);
//  row5 = (__m512i)_mm512_permute2f128_ps(__tt1, __tt5, 0x31);
//  row6 = (__m512i)_mm512_permute2f128_ps(__tt2, __tt6, 0x31);
//  row7 = (__m512i)_mm512_permute2f128_ps(__tt3, __tt7, 0x31);
}

void AVX512Util::Transpose8x8(__m512d &row0,
                              __m512d &row1,
                              __m512d &row2,
                              __m512d &row3,
                              __m512d &row4,
                              __m512d &row5,
                              __m512d &row6,
                              __m512d &row7) {
  // TODO: Convert
//  __m512 __t0, __t1, __t2, __t3, __t4, __t5, __t6, __t7;
//  __m512 __tt0, __tt1, __tt2, __tt3, __tt4, __tt5, __tt6, __tt7;
//  __t0 = _mm512_unpacklo_ps(row0, row1);
//  __t1 = _mm512_unpackhi_ps(row0, row1);
//  __t2 = _mm512_unpacklo_ps(row2, row3);
//  __t3 = _mm512_unpackhi_ps(row2, row3);
//  __t4 = _mm512_unpacklo_ps(row4, row5);
//  __t5 = _mm512_unpackhi_ps(row4, row5);
//  __t6 = _mm512_unpacklo_ps(row6, row7);
//  __t7 = _mm512_unpackhi_ps(row6, row7);
//  // Note: https://stackoverflow.com/questions/26983569/implications-of-using-mm-shuffle-ps-on-integer-vector
//  // As provided in above link, it is relatively safe/free to run float shuffles on integers(not other way around)
//  __tt0 = _mm512_shuffle_ps(__t0,__t2,_MM_SHUFFLE(1,0,1,0));
//  __tt1 = _mm512_shuffle_ps(__t0,__t2,_MM_SHUFFLE(3,2,3,2));
//  __tt2 = _mm512_shuffle_ps(__t1,__t3,_MM_SHUFFLE(1,0,1,0));
//  __tt3 = _mm512_shuffle_ps(__t1,__t3,_MM_SHUFFLE(3,2,3,2));
//  __tt4 = _mm512_shuffle_ps(__t4,__t6,_MM_SHUFFLE(1,0,1,0));
//  __tt5 = _mm512_shuffle_ps(__t4,__t6,_MM_SHUFFLE(3,2,3,2));
//  __tt6 = _mm512_shuffle_ps(__t5,__t7,_MM_SHUFFLE(1,0,1,0));
//  __tt7 = _mm512_shuffle_ps(__t5,__t7,_MM_SHUFFLE(3,2,3,2));
//  row0 = _mm512_permute2f128_ps(__tt0, __tt4, 0x20);
//  row1 = _mm512_permute2f128_ps(__tt1, __tt5, 0x20);
//  row2 = _mm512_permute2f128_ps(__tt2, __tt6, 0x20);
//  row3 = _mm512_permute2f128_ps(__tt3, __tt7, 0x20);
//  row4 = _mm512_permute2f128_ps(__tt0, __tt4, 0x31);
//  row5 = _mm512_permute2f128_ps(__tt1, __tt5, 0x31);
//  row6 = _mm512_permute2f128_ps(__tt2, __tt6, 0x31);
//  row7 = _mm512_permute2f128_ps(__tt3, __tt7, 0x31);
}

void AVX512Util::Transpose16x16(__m512i &row0, __m512i &row1, __m512i &row2, __m512i &row3,
                    __m512i &row4, __m512i &row5, __m512i &row6, __m512i &row7,
                    __m512i &row8, __m512i &row9, __m512i &row10, __m512i &row11,
                    __m512i &row12, __m512i &row13, __m512i &row14, __m512i &row15) {
  __m512i t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, ta, tb, tc, td, te, tf;
  __m512i tt0, tt1, tt2, tt3, tt4, tt5, tt6, tt7, tt8, tt9, tta, ttb, ttc, ttd, tte, ttf;
  t0 = _mm512_unpacklo_epi32(row0,row1); //   0  16   1  17   4  20   5  21   8  24   9  25  12  28  13  29
  t1 = _mm512_unpackhi_epi32(row0,row1); //   2  18   3  19   6  22   7  23  10  26  11  27  14  30  15  31
  t2 = _mm512_unpacklo_epi32(row2,row3); //  32  48  33  49 ...
  t3 = _mm512_unpackhi_epi32(row2,row3); //  34  50  35  51 ...
  t4 = _mm512_unpacklo_epi32(row4,row5); //  64  80  65  81 ...
  t5 = _mm512_unpackhi_epi32(row4,row5); //  66  82  67  83 ...
  t6 = _mm512_unpacklo_epi32(row6,row7); //  96 112  97 113 ...
  t7 = _mm512_unpackhi_epi32(row6,row7); //  98 114  99 115 ...
  t8 = _mm512_unpacklo_epi32(row8,row9); // 128 ...
  t9 = _mm512_unpackhi_epi32(row8,row9); // 130 ...
  ta = _mm512_unpacklo_epi32(row10,row11); // 160 ...
  tb = _mm512_unpackhi_epi32(row10,row11); // 162 ...
  tc = _mm512_unpacklo_epi32(row12,row13); // 196 ...
  td = _mm512_unpackhi_epi32(row12,row13); // 198 ...
  te = _mm512_unpacklo_epi32(row14,row15); // 228 ...
  tf = _mm512_unpackhi_epi32(row14,row15); // 230 ...

  tt0 = _mm512_unpacklo_epi64(t0,t2); //   0  16  32  48 ...
  tt1 = _mm512_unpackhi_epi64(t0,t2); //   1  17  33  49 ...
  tt2 = _mm512_unpacklo_epi64(t1,t3); //   2  18  34  49 ...
  tt3 = _mm512_unpackhi_epi64(t1,t3); //   3  19  35  51 ...
  tt4 = _mm512_unpacklo_epi64(t4,t6); //  64  80  96 112 ...
  tt5 = _mm512_unpackhi_epi64(t4,t6); //  65  81  97 114 ...
  tt6 = _mm512_unpacklo_epi64(t5,t7); //  66  82  98 113 ...
  tt7 = _mm512_unpackhi_epi64(t5,t7); //  67  83  99 115 ...
  tt8 = _mm512_unpacklo_epi64(t8,ta); // 128 144 160 176 ...
  tt9 = _mm512_unpackhi_epi64(t8,ta); // 129 145 161 178 ...
  tta = _mm512_unpacklo_epi64(t9,tb); // 130 146 162 177 ...
  ttb = _mm512_unpackhi_epi64(t9,tb); // 131 147 163 179 ...
  ttc = _mm512_unpacklo_epi64(tc,te); // 192 208 228 240 ...
  ttd = _mm512_unpackhi_epi64(tc,te); // 193 209 229 241 ...
  tte = _mm512_unpacklo_epi64(td,tf); // 194 210 230 242 ...
  ttf = _mm512_unpackhi_epi64(td,tf); // 195 211 231 243 ...

  t0 = _mm512_shuffle_i32x4(tt0, tt4, 0x88); //   0  16  32  48   8  24  40  56  64  80  96  112 ...
  t1 = _mm512_shuffle_i32x4(tt1, tt5, 0x88); //   1  17  33  49 ...
  t2 = _mm512_shuffle_i32x4(tt2, tt6, 0x88); //   2  18  34  50 ...
  t3 = _mm512_shuffle_i32x4(tt3, tt7, 0x88); //   3  19  35  51 ...
  t4 = _mm512_shuffle_i32x4(tt0, tt4, 0xdd); //   4  20  36  52 ...
  t5 = _mm512_shuffle_i32x4(tt1, tt5, 0xdd); //   5  21  37  53 ...
  t6 = _mm512_shuffle_i32x4(tt2, tt6, 0xdd); //   6  22  38  54 ...
  t7 = _mm512_shuffle_i32x4(tt3, tt7, 0xdd); //   7  23  39  55 ...
  t8 = _mm512_shuffle_i32x4(tt8, ttc, 0x88); // 128 144 160 176 ...
  t9 = _mm512_shuffle_i32x4(tt9, ttd, 0x88); // 129 145 161 177 ...
  ta = _mm512_shuffle_i32x4(tta, tte, 0x88); // 130 146 162 178 ...
  tb = _mm512_shuffle_i32x4(ttb, ttf, 0x88); // 131 147 163 179 ...
  tc = _mm512_shuffle_i32x4(tt8, ttc, 0xdd); // 132 148 164 180 ...
  td = _mm512_shuffle_i32x4(tt9, ttd, 0xdd); // 133 149 165 181 ...
  te = _mm512_shuffle_i32x4(tta, tte, 0xdd); // 134 150 166 182 ...
  tf = _mm512_shuffle_i32x4(ttb, ttf, 0xdd); // 135 151 167 183 ...

  row0 = _mm512_shuffle_i32x4(t0, t8, 0x88); //   0  16  32  48  64  80  96 112 ... 240
  row1 = _mm512_shuffle_i32x4(t1, t9, 0x88); //   1  17  33  49  66  81  97 113 ... 241
  row2 = _mm512_shuffle_i32x4(t2, ta, 0x88); //   2  18  34  50  67  82  98 114 ... 242
  row3 = _mm512_shuffle_i32x4(t3, tb, 0x88); //   3  19  35  51  68  83  99 115 ... 243
  row4 = _mm512_shuffle_i32x4(t4, tc, 0x88); //   4 ...
  row5 = _mm512_shuffle_i32x4(t5, td, 0x88); //   5 ...
  row6 = _mm512_shuffle_i32x4(t6, te, 0x88); //   6 ...
  row7 = _mm512_shuffle_i32x4(t7, tf, 0x88); //   7 ...
  row8 = _mm512_shuffle_i32x4(t0, t8, 0xdd); //   8 ...
  row9 = _mm512_shuffle_i32x4(t1, t9, 0xdd); //   9 ...
  row10 = _mm512_shuffle_i32x4(t2, ta, 0xdd); //  10 ...
  row11 = _mm512_shuffle_i32x4(t3, tb, 0xdd); //  11 ...
  row12 = _mm512_shuffle_i32x4(t4, tc, 0xdd); //  12 ...
  row13 = _mm512_shuffle_i32x4(t5, td, 0xdd); //  13 ...
  row14 = _mm512_shuffle_i32x4(t6, te, 0xdd); //  14 ...
  row15 = _mm512_shuffle_i32x4(t7, tf, 0xdd); //  15  31  47  63  79  96 111 127 ... 255
}

void AVX512Util::Transpose16x16(__m512 &row0, __m512 &row1, __m512 &row2, __m512 &row3,
                    __m512 &row4, __m512 &row5, __m512 &row6, __m512 &row7,
                    __m512 &row8, __m512 &row9, __m512 &row10, __m512 &row11,
                    __m512 &row12, __m512 &row13, __m512 &row14, __m512 &row15) {
  __m512 t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, ta, tb, tc, td, te, tf;
  __m512d tt0, tt1, tt2, tt3, tt4, tt5, tt6, tt7, tt8, tt9, tta, ttb, ttc, ttd, tte, ttf;
  // Note: The unpack section is converted to integer to void compiler error on Dev5.
  t0 = _mm512_unpacklo_ps(row0,row1); //   0  16   1  17   4  20   5  21   8  24   9  25  12  28  13  29
  t1 = _mm512_unpackhi_ps(row0,row1); //   2  18   3  19   6  22   7  23  10  26  11  27  14  30  15  31
  t2 = _mm512_unpacklo_ps(row2,row3); //  32  48  33  49 ...
  t3 = _mm512_unpackhi_ps(row2,row3); //  34  50  35  51 ...
  t4 = _mm512_unpacklo_ps(row4,row5); //  64  80  65  81 ...
  t5 = _mm512_unpackhi_ps(row4,row5); //  66  82  67  83 ...
  t6 = _mm512_unpacklo_ps(row6,row7); //  96 112  97 113 ...
  t7 = _mm512_unpackhi_ps(row6,row7); //  98 114  99 115 ...
  t8 = _mm512_unpacklo_ps(row8,row9); // 128 ...
  t9 = _mm512_unpackhi_ps(row8,row9); // 130 ...
  ta = _mm512_unpacklo_ps(row10,row11); // 160 ...
  tb = _mm512_unpackhi_ps(row10,row11); // 162 ...
  tc = _mm512_unpacklo_ps(row12,row13); // 196 ...
  td = _mm512_unpackhi_ps(row12,row13); // 198 ...
  te = _mm512_unpacklo_ps(row14,row15); // 228 ...
  tf = _mm512_unpackhi_ps(row14,row15); // 230 ...

  tt0 = _mm512_unpacklo_pd((__m512d)t0,(__m512d)t2); //   0  16  32  48 ...
  tt1 = _mm512_unpackhi_pd((__m512d)t0,(__m512d)t2); //   1  17  33  49 ...
  tt2 = _mm512_unpacklo_pd((__m512d)t1,(__m512d)t3); //   2  18  34  49 ...
  tt3 = _mm512_unpackhi_pd((__m512d)t1,(__m512d)t3); //   3  19  35  51 ...
  tt4 = _mm512_unpacklo_pd((__m512d)t4,(__m512d)t6); //  64  80  96 112 ...
  tt5 = _mm512_unpackhi_pd((__m512d)t4,(__m512d)t6); //  65  81  97 114 ...
  tt6 = _mm512_unpacklo_pd((__m512d)t5,(__m512d)t7); //  66  82  98 113 ...
  tt7 = _mm512_unpackhi_pd((__m512d)t5,(__m512d)t7); //  67  83  99 115 ...
  tt8 = _mm512_unpacklo_pd((__m512d)t8,(__m512d)ta); // 128 144 160 176 ...
  tt9 = _mm512_unpackhi_pd((__m512d)t8,(__m512d)ta); // 129 145 161 178 ...
  tta = _mm512_unpacklo_pd((__m512d)t9,(__m512d)tb); // 130 146 162 177 ...
  ttb = _mm512_unpackhi_pd((__m512d)t9,(__m512d)tb); // 131 147 163 179 ...
  ttc = _mm512_unpacklo_pd((__m512d)tc,(__m512d)te); // 192 208 228 240 ...
  ttd = _mm512_unpackhi_pd((__m512d)tc,(__m512d)te); // 193 209 229 241 ...
  tte = _mm512_unpacklo_pd((__m512d)td,(__m512d)tf); // 194 210 230 242 ...
  ttf = _mm512_unpackhi_pd((__m512d)td,(__m512d)tf); // 195 211 231 243 ...

  t0 = _mm512_shuffle_f32x4((__m512)tt0, (__m512)tt4, 0x88); //   0  16  32  48   8  24  40  56  64  80  96  112 ...
  t1 = _mm512_shuffle_f32x4((__m512)tt1, (__m512)tt5, 0x88); //   1  17  33  49 ...
  t2 = _mm512_shuffle_f32x4((__m512)tt2, (__m512)tt6, 0x88); //   2  18  34  50 ...
  t3 = _mm512_shuffle_f32x4((__m512)tt3, (__m512)tt7, 0x88); //   3  19  35  51 ...
  t4 = _mm512_shuffle_f32x4((__m512)tt0, (__m512)tt4, 0xdd); //   4  20  36  52 ...
  t5 = _mm512_shuffle_f32x4((__m512)tt1, (__m512)tt5, 0xdd); //   5  21  37  53 ...
  t6 = _mm512_shuffle_f32x4((__m512)tt2, (__m512)tt6, 0xdd); //   6  22  38  54 ...
  t7 = _mm512_shuffle_f32x4((__m512)tt3, (__m512)tt7, 0xdd); //   7  23  39  55 ...
  t8 = _mm512_shuffle_f32x4((__m512)tt8, (__m512)ttc, 0x88); // 128 144 160 176 ...
  t9 = _mm512_shuffle_f32x4((__m512)tt9, (__m512)ttd, 0x88); // 129 145 161 177 ...
  ta = _mm512_shuffle_f32x4((__m512)tta, (__m512)tte, 0x88); // 130 146 162 178 ...
  tb = _mm512_shuffle_f32x4((__m512)ttb, (__m512)ttf, 0x88); // 131 147 163 179 ...
  tc = _mm512_shuffle_f32x4((__m512)tt8, (__m512)ttc, 0xdd); // 132 148 164 180 ...
  td = _mm512_shuffle_f32x4((__m512)tt9, (__m512)ttd, 0xdd); // 133 149 165 181 ...
  te = _mm512_shuffle_f32x4((__m512)tta, (__m512)tte, 0xdd); // 134 150 166 182 ...
  tf = _mm512_shuffle_f32x4((__m512)ttb, (__m512)ttf, 0xdd); // 135 151 167 183 ...

  row0 = _mm512_shuffle_f32x4(t0, t8, 0x88); //   0  16  32  48  64  80  96 112 ... 240
  row1 = _mm512_shuffle_f32x4(t1, t9, 0x88); //   1  17  33  49  66  81  97 113 ... 241
  row2 = _mm512_shuffle_f32x4(t2, ta, 0x88); //   2  18  34  50  67  82  98 114 ... 242
  row3 = _mm512_shuffle_f32x4(t3, tb, 0x88); //   3  19  35  51  68  83  99 115 ... 243
  row4 = _mm512_shuffle_f32x4(t4, tc, 0x88); //   4 ...
  row5 = _mm512_shuffle_f32x4(t5, td, 0x88); //   5 ...
  row6 = _mm512_shuffle_f32x4(t6, te, 0x88); //   6 ...
  row7 = _mm512_shuffle_f32x4(t7, tf, 0x88); //   7 ...
  row8 = _mm512_shuffle_f32x4(t0, t8, 0xdd); //   8 ...
  row9 = _mm512_shuffle_f32x4(t1, t9, 0xdd); //   9 ...
  row10 = _mm512_shuffle_f32x4(t2, ta, 0xdd); //  10 ...
  row11 = _mm512_shuffle_f32x4(t3, tb, 0xdd); //  11 ...
  row12 = _mm512_shuffle_f32x4(t4, tc, 0xdd); //  12 ...
  row13 = _mm512_shuffle_f32x4(t5, td, 0xdd); //  13 ...
  row14 = _mm512_shuffle_f32x4(t6, te, 0xdd); //  14 ...
  row15 = _mm512_shuffle_f32x4(t7, tf, 0xdd); //  15  31  47  63  79  96 111 127 ... 255
}

__m512i AVX512Util::Reverse8(__m512i &v) {
  return _mm512_permutexvar_epi64(_mm512_setr_epi64(7, 6, 5, 4, 3, 2, 1, 0), v);
}

__m512d AVX512Util::Reverse8(__m512d &v) {
  return _mm512_permutexvar_pd(_mm512_setr_epi64(7, 6, 5, 4, 3, 2, 1, 0), v);
}

__m512i AVX512Util::Reverse16(__m512i &v) {
  return _mm512_permutexvar_epi32(_mm512_setr_epi32(15, 14, 13, 12, 11, 10, 9, 8,
                                                    7, 6, 5, 4, 3, 2, 1, 0), v);
}

__m512 AVX512Util::Reverse16(__m512 &v) {
  return _mm512_permutexvar_ps(_mm512_setr_epi32(15, 14, 13, 12, 11, 10, 9, 8,
                                                 7, 6, 5, 4, 3, 2, 1, 0), v);
}

#endif